CHAINID=6546
# Updated bootnode to use working RPC server
BOOTNODE="enr:-Jy4QKIeraMneTpcBY3RPSjcQ2fniEkvY1LrzPWWH_X2wfmuNhcTzsRwGVpfoEau6BFQAyT79gg_yU9ADhW8FnkRfaaGAZk0TjLvg2V0aMfGhODg-12AgmlkgnY0gmlwhBhlZJ-Jc2VjcDI1NmsxoQLOFbde-Rcgo1IfAIxF3e2p5Qiif9QMTFl5GddDeNgDsYRzbmFwwIN0Y3CCf5w"
IP=$(curl ifconfig.io -4)

# GPU Acceleration Configuration for High-Performance RPC (RTX 4000 SFF Ada - 20GB VRAM)
ENABLE_GPU=true
PREFERRED_GPU_TYPE=CUDA
GPU_MAX_BATCH_SIZE=200000
GPU_MAX_MEMORY_USAGE=10737418240
GPU_MEMORY_FRACTION=0.5
GPU_HASH_WORKERS=16
GPU_SIGNATURE_WORKERS=16
GPU_TX_WORKERS=16
GPU_ENABLE_PIPELINING=true

# Hybrid Processing Configuration with AI Coordination
ENABLE_HYBRID_PROCESSING=true
GPU_THRESHOLD=1000
CPU_GPU_RATIO=0.5
ADAPTIVE_LOAD_BALANCING=true
PERFORMANCE_MONITORING=true
MAX_CPU_UTILIZATION=0.85
MAX_GPU_UTILIZATION=0.50
THROUGHPUT_TARGET=2000000

# Memory Management (RTX 4000 SFF Ada - 20GB Total)
MAX_MEMORY_USAGE=68719476736
GPU_MEMORY_RESERVATION=10737418240
AI_MEMORY_RESERVATION=8589934592
MEMORY_BUFFER=2147483648

# Performance Optimization for AI Sharing
GPU_DEVICE_COUNT=1
GPU_LOAD_BALANCE_STRATEGY=ai_optimized
AI_GPU_COORDINATION=true
ENABLE_CUDA_MPS=true

# CUDA Environment Configuration (Auto-activated by node-start.sh)
CUDA_PATH=/usr/local/cuda
CUDA_VISIBLE_DEVICES=0

# AI-Powered Load Balancing Configuration (vLLM + TinyLlama 1.1B)
ENABLE_AI_LOAD_BALANCING=true
LLM_ENDPOINT=http://localhost:8000/v1/completions
LLM_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
LLM_TIMEOUT_SECONDS=1
AI_UPDATE_INTERVAL_MS=500
AI_HISTORY_SIZE=100
AI_LEARNING_RATE=0.15
AI_CONFIDENCE_THRESHOLD=0.75
AI_ENABLE_LEARNING=true
AI_ENABLE_PREDICTIONS=true
AI_FAST_MODE=true
VLLM_GPU_MEMORY_UTILIZATION=0.4
VLLM_MAX_MODEL_LEN=4096
